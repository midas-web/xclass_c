(xclass_2) ccy@ccy--pc:~/pytrochproject/xclass/XClass/scripts$ ./run.sh 0 20News
{'dataset_name': '20News', 'random_state': 42, 'lm_type': 'bbu', 'vocab_min_occurrence': 5, 'layer': 12}

### Dataset statistics for raw_txt: ###
# of documents is: 17871
Document max length: 18171 (words)
Document average length: 310.8175256001343 (words)
Document length std: 614.3708170678227 (words)
#######################################
Cleaned 0 html links

### Dataset statistics for cleaned_txt: ###
# of documents is: 17871
Document max length: 17686 (words)
Document average length: 281.00419674332716 (words)
Document length std: 633.420660216991 (words)
#######################################
Finish reading data
  0%|                                                                                                           | 0/17871 [00:00<?, ?it/s]101 102
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 17871/17871 [01:21<00:00, 219.38it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████| 17871/17871 [14:15<00:00, 20.89it/s]
{'dataset_name': '20News', 'random_state': 42, 'lm_type': 'bbu', 'layer': 12, 'T': 100, 'attention_mechanism': 'mixture'}
Finish reading data
['computer', 'sports', 'science', 'politics', 'religion']
['science', 'scientific', 'scientists', 'scientist', 'scientifically', 'sciences', 'research', 'physics', 'researchers', 'researcher', 'laboratory', 'lab', 'mechanics', 'physicist', 'chemistry', 'laboratories', 'studies', 'labs', 'physicists', 'biology', 'experiments', 'biologists', 'chemists', 'nontechnical', 'experimentally', 'experimenters', 'observational', 'engineering', 'inventors', 'theoretical', 'experimental', 'academics', 'experiment', 'discoveries', 'discovery', 'astronomers', 'fermilab', 'geologists', 'theory', 'theorizing', 'empirical', 'methodological', 'empirically', 'chemist', 'experimentation', 'studied']
100%|███████████████████████████████████████████████████████████████████████████████████████████████| 17871/17871 [20:09<00:00, 14.77it/s]
Finish getting document representations
{'dataset_name': '20News', 'pca': 64, 'cluster_method': 'gmm', 'lm_type': 'bbu-12', 'document_repr_type': 'mixture-100', 'random_state': 42}
pca64.clusgmm.bbu-12.mixture-100.42
../data/intermediate_data/20News
['computer', 'sports', 'science', 'politics', 'religion']
Explained variance: 0.7301839713690013
{'dataset': '20News', 'stage': 'Rep', 'suffix': 'bbu-12-mixture-100'}
--------------------------------------------------------------------------------Evaluating--------------------------------------------------------------------------------
[[4691   39  133   23    5]
 [ 307 3252  166  212   42]
 [1380  222 1739  563   48]
 [  68  112  196 1771  478]
 [  43   25  157  200 1999]]
F1 macro: 0.7364363831936181
F1 micro: 0.7527278831626658
{'dataset': '20News', 'stage': 'Align', 'suffix': 'pca64.clusgmm.bbu-12.mixture-100.42'}
--------------------------------------------------------------------------------Evaluating--------------------------------------------------------------------------------
[[4752   15   98   24    2]
 [ 201 3329  203  228   18]
 [1506   34 1936  462   14]
 [  53   17  107 2310  138]
 [  48    1  134  422 1819]]
F1 macro: 0.7839864180575367
F1 micro: 0.7915617480834872
{'dataset_name': '20News', 'suffix': 'pca64.clusgmm.bbu-12.mixture-100.42', 'confidence_threshold': 0.5}
--------------------------------------------------------------------------------Evaluating--------------------------------------------------------------------------------
[[2868    0    8    1    0]
 [   1 1698    7   40    1]
 [ 410    0 1187  145    0]
 [   1    0    7 1417   14]
 [   0    0   30  120  980]]
F1 macro: 0.9064794423318208
F1 micro: 0.9121432568550645
03/10/2022 18:50:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
03/10/2022 18:50:22 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /home/ccy/.cache/torch/transformers/tmpj9nqlbka
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████| 433/433 [00:00<00:00, 212kB/s]
03/10/2022 18:50:23 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at /home/ccy/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/10/2022 18:50:23 - INFO - transformers.file_utils -   creating metadata file for /home/ccy/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/10/2022 18:50:23 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/ccy/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/10/2022 18:50:23 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "20News",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

03/10/2022 18:50:24 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/ccy/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/10/2022 18:50:24 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

03/10/2022 18:50:25 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /home/ccy/.cache/torch/transformers/tmpqvfwxkrx
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████| 213k/213k [00:01<00:00, 148kB/s]
03/10/2022 18:50:28 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /home/ccy/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/10/2022 18:50:28 - INFO - transformers.file_utils -   creating metadata file for /home/ccy/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/10/2022 18:50:28 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/ccy/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/10/2022 18:50:29 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ccy/.cache/torch/transformers/tmpb7h6qlq3
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 436M/436M [00:53<00:00, 8.08MB/s]
03/10/2022 18:51:24 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin in cache at /home/ccy/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/10/2022 18:51:24 - INFO - transformers.file_utils -   creating metadata file for /home/ccy/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/10/2022 18:51:24 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /home/ccy/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/10/2022 18:51:26 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/10/2022 18:51:26 - WARNING - transformers.modeling_utils -   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/10/2022 18:51:28 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir=None, config_name='', data_dir='../data/datasets', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=100000, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-cased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='../models/bert-base-cased_pca64.clusgmm.bbu-12.mixture-100.42.0.5', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=32, save_steps=-1, seed=42, server_ip='', server_port='', task_name='20News', test_suffix='', tokenizer_name='', train_suffix='pca64.clusgmm.bbu-12.mixture-100.42.0.5', warmup_steps=0, weight_decay=0.0)
03/10/2022 18:51:28 - INFO - __main__ -   Creating features from dataset file at ../data/datasets
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   *** Example ***
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   guid: train-0
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 11336, 1731, 1106, 12120, 8517, 22583, 149, 26911, 119, 119, 119, 1541, 161, 3128, 11613, 1200, 157, 11607, 1683, 122, 119, 122, 153, 2162, 1580, 4345, 10117, 1724, 1130, 3342, 113, 7504, 5487, 114, 6474, 1986, 117, 146, 112, 182, 1136, 2157, 1115, 9528, 6660, 1110, 1103, 1436, 1236, 1106, 4267, 8517, 22583, 1122, 112, 188, 1304, 1662, 1106, 2754, 139, 1830, 1107, 1211, 2740, 119, 1109, 1553, 1110, 1115, 1987, 119, 151, 1144, 1872, 170, 107, 1631, 107, 1111, 1184, 1110, 1105, 1184, 2762, 112, 189, 149, 2137, 119, 1188, 2502, 1121, 1201, 1104, 2541, 119, 1302, 14516, 13166, 15741, 1169, 1801, 1115, 119, 7595, 117, 1199, 1156, 1840, 1987, 119, 151, 170, 107, 186, 6718, 2158, 107, 1105, 170, 19515, 5613, 1140, 1104, 1774, 1106, 1294, 170, 3613, 171, 8474, 119, 2009, 1202, 1128, 1341, 1119, 1156, 1129, 1270, 170, 186, 6718, 2158, 136, 1109, 186, 6718, 8770, 1274, 112, 189, 1202, 8708, 119, 1220, 185, 5658, 185, 5658, 1833, 1167, 8074, 5715, 107, 1142, 1110, 149, 26911, 117, 2059, 1143, 117, 146, 112, 1396, 1562, 1122, 1242, 1551, 119, 1109, 8074, 5715, 4597, 112, 189, 8026, 119, 1284, 112, 1325, 7299, 1122, 1208, 119, 107, 2907, 117, 1110, 1987, 119, 151, 112, 188, 2415, 1593, 7097, 6970, 1106, 12770, 149, 26911, 4420, 136, 146, 1274, 112, 189, 1221, 1251, 1137, 1582, 15680, 1596, 10690, 1116, 1150, 4218, 1142, 4844, 119, 1220, 1132, 1932, 15175, 1116, 119, 1302, 117, 1119, 1674, 1136, 7097, 7299, 149, 2137, 4420, 119, 1438, 117, 1107, 1199, 2192, 1104, 1103, 1583, 117, 1128, 1274, 112, 189, 1444, 1106, 1129, 1227, 1112, 1126, 149, 2137, 107, 9131, 107, 1106, 1267, 170, 1415, 1295, 1104, 149, 2137, 4420, 2647, 1194, 1240, 1701, 119, 10470, 1103, 3321, 2463, 1104, 1223, 7168, 25566, 4863, 117, 1137, 1582, 15680, 3681, 8107, 1523, 23487, 6006, 1104, 1103, 3653, 1198, 1164, 1451, 1285, 1107, 1147, 2366, 5660, 119, 1987, 119, 151, 119, 1500, 1143, 1115, 1314, 1214, 117, 1119, 1850, 1206, 123, 1105, 126, 4420, 170, 1989, 1106, 1103, 149, 2137, 18137, 119, 119, 119, 1105, 1119, 1110, 1136, 1103, 1178, 1137, 1582, 15680, 3681, 1107, 1103, 1411, 119, 2421, 112, 188, 1474, 1115, 1178, 123, 1234, 1679, 1989, 2140, 1138, 149, 2137, 119, 1337, 2086, 1120, 1103, 1304, 5867, 9377, 1234, 1107, 1412, 1411, 113, 1105, 5670, 1298, 114, 3689, 1523, 2016, 23487, 6006, 1104, 149, 2137, 1451, 1214, 119, 24930, 1181, 1107, 1103, 13918, 1150, 1127, 11534, 1118, 24928, 11955, 17246, 117, 187, 4638, 10161, 2430, 17246, 117, 15175, 1116, 117, 3576, 117, 1105, 1128, 1169, 1267, 1184, 1912, 1104, 2463, 1195, 1138, 119, 1302, 4608, 1198, 1164, 10565, 1107, 1411, 7572, 3520, 1126, 149, 2137, 5351, 119, 1124, 4431, 1211, 4420, 1106, 149, 2137, 18137, 117, 1133, 1107, 6122, 2740, 1119, 8165, 1103, 5351, 1113, 15683, 2411, 1106, 20220, 1103, 3290, 113, 1107, 1211, 2740, 117, 1106, 1103, 4257, 114, 119, 4345, 1110, 5663, 1165, 1119, 2231, 1115, 1211, 149, 2137, 18137, 1132, 15175, 1116, 119, 7504, 119, 7504, 140, 119, 5487, 158, 119, 140, 119, 9167, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   *** Example ***
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   guid: train-1
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 8540, 1111, 170, 3995, 146, 1108, 1912, 1104, 1544, 2903, 1715, 9337, 1314, 1480, 1105, 1141, 1104, 1103, 9038, 1108, 1164, 1142, 3995, 1107, 156, 119, 143, 119, 1150, 2790, 170, 1555, 1104, 11950, 3252, 1111, 1672, 8131, 119, 146, 112, 182, 2785, 1612, 1117, 1271, 1110, 1987, 119, 2392, 16513, 2605, 2895, 113, 188, 1643, 136, 114, 1137, 1601, 1106, 1115, 119, 2966, 2256, 1950, 2824, 1142, 136, 146, 112, 173, 1176, 1106, 1243, 1117, 5663, 1271, 1105, 4134, 2179, 1295, 1191, 1936, 119, 5749, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   *** Example ***
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   guid: train-2
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 11336, 1731, 1164, 170, 5683, 1788, 1107, 3501, 13280, 13601, 22176, 1844, 136, 1130, 3342, 113, 4926, 148, 3484, 1377, 114, 6474, 146, 1138, 1151, 14255, 18408, 26938, 1142, 1911, 1111, 1199, 1159, 1112, 1218, 119, 146, 1821, 1136, 170, 3995, 117, 1133, 1139, 1676, 1110, 170, 7439, 1105, 146, 1221, 170, 1974, 1104, 8114, 1105, 13318, 119, 1109, 1553, 1303, 1217, 1115, 8114, 1105, 13318, 1202, 1136, 3166, 1106, 1243, 4809, 2212, 1112, 1277, 1112, 1234, 1796, 1103, 2657, 9545, 119, 1188, 1110, 170, 9020, 1298, 1111, 1126, 10294, 26595, 1116, 117, 1133, 146, 1821, 1612, 1128, 1132, 1113, 1106, 1380, 119, 1249, 170, 7454, 117, 146, 1593, 1309, 1243, 4809, 1932, 117, 1165, 1380, 16358, 11604, 13645, 1361, 1110, 1280, 1213, 117, 146, 1719, 1274, 112, 189, 1243, 1122, 1120, 1155, 1137, 1243, 170, 1304, 10496, 1692, 119, 1332, 146, 1202, 1243, 1541, 4809, 117, 1122, 1110, 1579, 1380, 5283, 119, 1188, 1108, 1136, 1103, 2820, 1165, 146, 1108, 1107, 2657, 1278, 117, 2521, 1113, 185, 24795, 1116, 119, 146, 1309, 1125, 3247, 9302, 1991, 117, 1105, 1165, 146, 1355, 1113, 1103, 185, 24795, 12551, 146, 2840, 1276, 1991, 15034, 1158, 1155, 12072, 1104, 20342, 7806, 1115, 1139, 1404, 1108, 1136, 2407, 1111, 119, 153, 24795, 1116, 1111, 1143, 1108, 1210, 4600, 1808, 1104, 6946, 117, 1105, 146, 1125, 170, 21359, 8223, 1104, 9377, 1165, 146, 1261, 1103, 1509, 12211, 106, 146, 1341, 1184, 5940, 1110, 1115, 1219, 2013, 117, 1105, 2894, 117, 1195, 1132, 7480, 5490, 1106, 1207, 1614, 117, 1105, 1195, 1138, 1103, 4400, 9535, 1106, 1172, 117, 1177, 1115, 1224, 1113, 117, 1165, 7964, 1114, 1380, 117, 1122, 1110, 1167, 2620, 170, 1231, 7401, 1111, 1366, 117, 1177, 1195, 2239, 1114, 1122, 1218, 1105, 1243, 170, 10496, 6946, 119, 146, 1274, 112, 189, 1341, 1122, 1110, 1115, 1103, 11650, 1449, 1110, 177, 16726, 1181, 1146, 1107, 1251, 1236, 119, 2907, 117, 1274, 112, 189, 5042, 1115, 1103, 2704, 16812, 1110, 1304, 1472, 1121, 1103, 1313, 117, 1105, 1195, 3564, 170, 1974, 1104, 1115, 1213, 119, 1557, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   *** Example ***
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   guid: train-3
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 11336, 10208, 1104, 9292, 112, 188, 2552, 9641, 136, 1130, 3342, 113, 5316, 7949, 114, 6474, 1135, 112, 188, 1136, 1126, 2552, 9641, 119, 1135, 1110, 1270, 170, 1246, 5220, 119, 1398, 8114, 1309, 138, 188, 25392, 19463, 136, 1109, 188, 25392, 19463, 1110, 1103, 1376, 15775, 1115, 17976, 1113, 1103, 1322, 1104, 1103, 184, 11990, 16260, 119, 1247, 1132, 1145, 191, 27547, 7050, 188, 25392, 5886, 1115, 3032, 1105, 176, 10941, 27427, 1116, 1132, 1155, 1315, 4509, 1114, 119, 4345, 10117, 151, 1495, 4538, 3190, 2101, 107, 156, 2391, 26265, 1110, 1103, 22572, 12788, 1785, 1104, 1103, 1107, 7854, 18465, 117, 1105, 1122, 1110, 10499, 2365, 1106, 7906, 1122, 1315, 1770, 119, 107, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   *** Example ***
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   guid: train-4
03/10/2022 18:52:07 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 150, 3663, 9272, 9637, 3663, 15393, 2162, 22680, 12480, 160, 12150, 3048, 16625, 14697, 1708, 146, 2323, 1344, 1278, 1107, 1103, 1727, 7438, 117, 1756, 1298, 1107, 1103, 1346, 2253, 112, 188, 117, 1105, 146, 2676, 170, 113, 1353, 2944, 114, 8010, 1104, 170, 4020, 6946, 117, 1107, 1134, 1234, 1872, 1143, 2225, 2897, 1176, 7152, 1113, 1147, 3470, 119, 1188, 3879, 1882, 1106, 1314, 1178, 170, 1374, 1552, 117, 1105, 146, 1274, 112, 189, 9148, 2256, 7516, 1251, 1168, 8006, 119, 146, 3166, 1106, 9148, 3455, 4476, 1115, 1142, 1108, 2475, 1106, 1138, 1151, 14837, 1107, 2731, 117, 1133, 146, 1274, 112, 189, 1221, 1111, 1612, 119, 1438, 117, 146, 1138, 1151, 8193, 1290, 1173, 1164, 1142, 119, 15859, 1138, 1251, 4133, 1164, 1184, 1142, 1547, 1138, 1151, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
03/10/2022 18:52:07 - INFO - __main__ -   Saving features into cached file ../data/datasets/20News_pca64.clusgmm.bbu-12.mixture-100.42.0.5/cached_train_bert-base-cased_512
03/10/2022 18:52:13 - INFO - __main__ -   ***** Running training *****
03/10/2022 18:52:13 - INFO - __main__ -     Num examples = 8935
03/10/2022 18:52:13 - INFO - __main__ -     Num Epochs = 3
03/10/2022 18:52:13 - INFO - __main__ -     Instantaneous batch size per GPU = 32
03/10/2022 18:52:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
03/10/2022 18:52:13 - INFO - __main__ -     Gradient Accumulation steps = 1
03/10/2022 18:52:13 - INFO - __main__ -     Total optimization steps = 840
Iteration:   0%|                                                                                                  | 0/280 [00:00<?, ?it/s]
Epoch:   0%|                                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "train_text_classifier.py", line 681, in <module>
    main()
  File "train_text_classifier.py", line 635, in main
    global_step, tr_loss = train(args, train_dataset, model, tokenizer)
  File "train_text_classifier.py", line 217, in train
    outputs = model(**inputs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/transformers/modeling_bert.py", line 1267, in forward
    output_hidden_states=output_hidden_states,
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/transformers/modeling_bert.py", line 762, in forward
    output_hidden_states=output_hidden_states,
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/transformers/modeling_bert.py", line 439, in forward
    output_attentions,
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/transformers/modeling_bert.py", line 371, in forward
    hidden_states, attention_mask, head_mask, output_attentions=output_attentions,
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/transformers/modeling_bert.py", line 315, in forward
    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/transformers/modeling_bert.py", line 231, in forward
    mixed_key_layer = self.key(hidden_states)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/ccy/anaconda3/envs/xclass_2/lib/python3.6/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 5.80 GiB total capacity; 4.54 GiB already allocated; 22.38 MiB free; 4.58 GiB reserved in total by PyTorch)








==============================second try================================




(xclass_2) ccy@ccy--pc:~/pytrochproject/xclass/XClass/scripts$ ./run.sh 0 20News{'dataset_name': '20News', 'random_state': 42, 'lm_type': 'bbu', 'vocab_min_occurrence': 5, 'layer': 12}

### Dataset statistics for raw_txt: ###
# of documents is: 17871
Document max length: 18171 (words)
Document average length: 310.8175256001343 (words)
Document length std: 614.3708170678227 (words)
#######################################
Cleaned 0 html links

### Dataset statistics for cleaned_txt: ###
# of documents is: 17871
Document max length: 17686 (words)
Document average length: 281.00419674332716 (words)
Document length std: 633.420660216991 (words)
#######################################
Finish reading data
  0%|                                                 | 0/17871 [00:00<?, ?it/s]101 102
100%|████████████████████████████████████| 17871/17871 [01:24<00:00, 211.82it/s]
100%|█████████████████████████████████████| 17871/17871 [14:27<00:00, 20.61it/s]
{'dataset_name': '20News', 'random_state': 42, 'lm_type': 'bbu', 'layer': 12, 'T': 100, 'attention_mechanism': 'mixture'}
Finish reading data
['computer', 'sports', 'science', 'politics', 'religion']
['science', 'scientific', 'scientists', 'scientist', 'scientifically', 'sciences', 'research', 'physics', 'researchers', 'researcher', 'laboratory', 'lab', 'mechanics', 'physicist', 'chemistry', 'laboratories', 'studies', 'labs', 'physicists', 'biology', 'experiments', 'biologists', 'chemists', 'nontechnical', 'experimentally', 'experimenters', 'observational', 'engineering', 'inventors', 'theoretical', 'experimental', 'academics', 'experiment', 'discoveries', 'discovery', 'astronomers', 'fermilab', 'geologists', 'theory', 'theorizing', 'empirical', 'methodological', 'empirically', 'chemist', 'experimentation', 'studied']
100%|█████████████████████████████████████| 17871/17871 [20:54<00:00, 14.25it/s]
Finish getting document representations
{'dataset_name': '20News', 'pca': 64, 'cluster_method': 'gmm', 'lm_type': 'bbu-12', 'document_repr_type': 'mixture-100', 'random_state': 42}
pca64.clusgmm.bbu-12.mixture-100.42
../data/intermediate_data/20News
['computer', 'sports', 'science', 'politics', 'religion']
Explained variance: 0.7301839713690013
{'dataset': '20News', 'stage': 'Rep', 'suffix': 'bbu-12-mixture-100'}
--------------------------------------------------------------------------------Evaluating--------------------------------------------------------------------------------
[[4691   39  133   23    5]
 [ 307 3252  166  212   42]
 [1380  222 1739  563   48]
 [  68  112  196 1771  478]
 [  43   25  157  200 1999]]
F1 macro: 0.7364363831936181
F1 micro: 0.7527278831626658
{'dataset': '20News', 'stage': 'Align', 'suffix': 'pca64.clusgmm.bbu-12.mixture-100.42'}
--------------------------------------------------------------------------------Evaluating--------------------------------------------------------------------------------
[[4752   15   98   24    2]
 [ 201 3329  203  228   18]
 [1506   34 1936  462   14]
 [  53   17  107 2310  138]
 [  48    1  134  422 1819]]
F1 macro: 0.7839864180575367
F1 micro: 0.7915617480834872
{'dataset_name': '20News', 'suffix': 'pca64.clusgmm.bbu-12.mixture-100.42', 'confidence_threshold': 0.5}
--------------------------------------------------------------------------------Evaluating--------------------------------------------------------------------------------
[[2868    0    8    1    0]
 [   1 1698    7   40    1]
 [ 410    0 1187  145    0]
 [   1    0    7 1417   14]
 [   0    0   30  120  980]]
F1 macro: 0.9064794423318208
F1 micro: 0.9121432568550645
Traceback (most recent call last):
  File "prepare_text_classifer_training.py", line 79, in <module>
    main(args.dataset_name, args.suffix, args.confidence_threshold)
  File "prepare_text_classifer_training.py", line 68, in main
    write_to_dir(text, classes, dataset_name, f"{suffix}.{confidence_threshold}")
  File "prepare_text_classifer_training.py", line 21, in write_to_dir
    assert False, f"{os.path.join(DATA_FOLDER_PATH, new_dataset_name)} exists."
AssertionError: ../data/datasets/20News_pca64.clusgmm.bbu-12.mixture-100.42.0.5 exists.


